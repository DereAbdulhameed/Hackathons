{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DereAbdulhameed/Hackathons/blob/main/Umoja%20Hack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install geopandas"
      ],
      "metadata": {
        "id": "f9r368y2krC8",
        "outputId": "355b5b52-94b0-4413-ed5d-336e76f9036d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "f9r368y2krC8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting geopandas\n",
            "  Downloading geopandas-0.12.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyproj>=2.6.1.post1\n",
            "  Downloading pyproj-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from geopandas) (1.4.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from geopandas) (23.0)\n",
            "Requirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.9/dist-packages (from geopandas) (2.0.1)\n",
            "Collecting fiona>=1.8\n",
            "  Downloading Fiona-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (63.4.3)\n",
            "Collecting munch>=2.3.2\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (22.2.0)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.9/dist-packages (from fiona>=1.8->geopandas) (8.1.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->geopandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.0->geopandas) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.15.0)\n",
            "Installing collected packages: pyproj, munch, cligj, click-plugins, fiona, geopandas\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.1 geopandas-0.12.2 munch-2.5.0 pyproj-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e2ade8cd",
      "metadata": {
        "id": "e2ade8cd",
        "outputId": "9958c77b-a1b6-44be-a682-7746cda8755a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-8bcbdd013d1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Load files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0msamplesubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SampleSubmission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train.csv'"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import folium\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "pd.options.display.float_format = '{:.5f}'.format\n",
        "pd.options.display.max_rows = None\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# Set seed for reproducability\n",
        "SEED = 2023\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "DATA_PATH = ''\n",
        "# Load files\n",
        "train = pd.read_csv(os.path.join(DATA_PATH, 'Train.csv'))\n",
        "test = pd.read_csv(os.path.join(DATA_PATH, 'Test.csv'))\n",
        "samplesubmission = pd.read_csv(os.path.join(DATA_PATH, 'SampleSubmission.csv'))\n",
        "\n",
        "# Preview train dataset\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2b45a4",
      "metadata": {
        "id": "1c2b45a4"
      },
      "outputs": [],
      "source": [
        "train.fillna(train.mean(), inplace=True)\n",
        "test.fillna(test.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd45d79",
      "metadata": {
        "id": "0fd45d79"
      },
      "outputs": [],
      "source": [
        "# Sample a unique location and visualize its emissions across the years\n",
        "train.latitude, train.longitude = round(train.latitude, 2), round(train.longitude, 2)\n",
        "sample_loc = train[(train.latitude == -23.73) & (train.longitude == 28.77)]\n",
        "\n",
        "# Plot a line plot\n",
        "sns.set_style('darkgrid')\n",
        "fig, axes = plt.subplots(nrows = 4, ncols = 1, figsize = (13, 10))\n",
        "fig.suptitle('Co2 emissions for location lat -23.75 lon 28.75', y=1.02, fontsize = 15)\n",
        "\n",
        "for ax, data, year, color, in zip(axes.flatten(), sample_loc, sample_loc.year.unique(), ['#882255','#332288', '#999933' , 'orangered']):\n",
        "  df = sample_loc[sample_loc.year == year]\n",
        "  sns.lineplot(df.week_no, df.emission, ax = ax, label = year, color = color)\n",
        "plt.legend()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbf7dd5",
      "metadata": {
        "id": "3fbf7dd5"
      },
      "outputs": [],
      "source": [
        "# Combine train and test for easy visualisation\n",
        "train_coords = train.drop_duplicates(subset = ['latitude', 'longitude'])\n",
        "test_coords = test.drop_duplicates(subset = ['latitude', 'longitude'])\n",
        "train_coords['set_type'], test_coords['set_type'] = 'train', 'test'\n",
        "\n",
        "all_data = pd.concat([train_coords, test_coords], ignore_index = True)\n",
        "# Create point geometries\n",
        "\n",
        "geometry = gpd.points_from_xy(all_data.longitude, all_data.latitude)\n",
        "geo_df = gpd.GeoDataFrame(\n",
        "    all_data[[\"latitude\", \"longitude\", \"set_type\"]], geometry=geometry\n",
        ")\n",
        "\n",
        "# Preview the geopandas df\n",
        "geo_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "226a3b19",
      "metadata": {
        "id": "226a3b19"
      },
      "outputs": [],
      "source": [
        "# Examples of feature engineering - Aggregations, cumulative differences, moving averages ...\n",
        "# Lets explore the rolling mean\n",
        "# First we create a unique location from lat lon\n",
        "train['location'] = [str(x) + '_' + str(y) for x, y in zip(train.latitude, train.longitude)]\n",
        "\n",
        "# Filter based on one location\n",
        "example_loc = train[train.location == '-23.73_28.77']\n",
        "\n",
        "# Calculate rolling mean for SulphurDioxide_SO2_column_number_density_amf with a window of 2 weeks\n",
        "rolling_mean = example_loc['SulphurDioxide_SO2_column_number_density_amf'].rolling(window = 2).mean()\n",
        "\n",
        "# Visualise rolling mean\n",
        "plt.figure(figsize = (15, 7))\n",
        "rolling_mean.plot()\n",
        "plt.title('Rolling mean with a window of 2 weeks for SulphurDioxide_SO2_column_number_density_amf', y = 1.02, fontsize = 15)\n",
        "plt.xlabel('week', y = 1.05, fontsize = 13)\n",
        "plt.ylabel('SulphurDioxide_SO2_column_number_density_amf', x = 1.05, fontsize = 13)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45045f1",
      "metadata": {
        "id": "e45045f1"
      },
      "outputs": [],
      "source": [
        "# Generate the above feature - rolling mean for all locations for both the train and test\n",
        "\n",
        "# Feature engineering train\n",
        "train_roll_mean = train.sort_values(by = ['location', 'year', 'week_no']).groupby(['location'])[train.columns[5:].tolist()].rolling(window = 2).mean().reset_index()\n",
        "train_roll_mean.drop(['level_1', 'emission', 'location'], axis = 1, inplace = True)\n",
        "train_roll_mean.columns = [col + '_roll_mean' for col in train_roll_mean.columns]\n",
        "\n",
        "# Feature engineering test\n",
        "test.latitude, test.longitude = round(test.latitude, 2), round(test.longitude, 2)\n",
        "test['location'] = [str(x) + '_' + str(y) for x, y in zip(test.latitude, test.longitude)]\n",
        "test_roll_mean = test.sort_values(by = ['location', 'year', 'week_no']).groupby(['location'])[test.columns[5:].tolist()].rolling(window = 2).mean().reset_index()\n",
        "test_roll_mean.drop(['level_1', 'location'], axis = 1, inplace = True)\n",
        "test_roll_mean.columns =  [col + '_roll_mean' for col in test_roll_mean.columns]\n",
        "test_roll_mean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8898038",
      "metadata": {
        "id": "a8898038"
      },
      "outputs": [],
      "source": [
        "# Merge engineered features with train and test set\n",
        "\n",
        "#Train\n",
        "train_eng = train.sort_values(by = ['location', 'year', 'week_no'], ignore_index = True).merge(train_roll_mean, how = 'left',\n",
        "                                                                                               left_index=True, right_index=True)\n",
        "\n",
        "# Test\n",
        "test_eng = test.sort_values(by = ['location', 'year', 'week_no'], ignore_index = True).merge(test_roll_mean, how = 'left',\n",
        "                                                                                               left_index=True, right_index=True)\n",
        "\n",
        "# Preview engineered test set\n",
        "test_eng.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d236c3de",
      "metadata": {
        "id": "d236c3de"
      },
      "outputs": [],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2c2fa8f",
      "metadata": {
        "id": "a2c2fa8f"
      },
      "outputs": [],
      "source": [
        "aggs = {}\n",
        "aggs['SulphurDioxide_SO2_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['SulphurDioxide_SO2_slant_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['SulphurDioxide_SO2_column_number_density_15km'] = ['sum','max','min','mean','std']\n",
        "aggs['CarbonMonoxide_CO_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['CarbonMonoxide_H2O_column_number_density'] = ['sum','max','min','mean','std']\n",
        "\n",
        "aggs['NitrogenDioxide_NO2_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['NitrogenDioxide_stratospheric_NO2_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['NitrogenDioxide_NO2_slant_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['NitrogenDioxide_tropospheric_NO2_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['Formaldehyde_tropospheric_HCHO_column_number_density'] = ['sum','max','min','mean','std']\n",
        "aggs['Formaldehyde_tropospheric_HCHO_column_number_density_amf'] = ['sum','max','min','mean','std']\n",
        "aggs['Formaldehyde_HCHO_slant_column_number_density'] = ['sum','max','min','mean','std']\n",
        "\n",
        "\n",
        "aggs['SulphurDioxide_cloud_fraction'] = ['max','min','mean','std']\n",
        "aggs['NitrogenDioxide_cloud_fraction'] = ['max','min','mean','std']\n",
        "aggs['Formaldehyde_cloud_fraction'] = ['max','min','mean','std']\n",
        "aggs['Ozone_cloud_fraction'] = ['max','min','mean','std']\n",
        "\n",
        "\n",
        "aggs['Ozone_O3_column_number_density'] = ['max','min','mean','std']\n",
        "aggs['Ozone_O3_column_number_density_amf'] = ['sum','max','min','mean','std']\n",
        "aggs['Ozone_O3_slant_column_number_density'] = ['sum','max','min','mean','std']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e219426",
      "metadata": {
        "id": "9e219426"
      },
      "outputs": [],
      "source": [
        "agg_trans = train_eng.groupby(['location']).agg(aggs)\n",
        "agg_trans.columns = ['_'.join(col).strip() for col in agg_trans.columns.values]\n",
        "agg_trans.reset_index(inplace=True)\n",
        "\n",
        "df = (train_eng.groupby('location')\n",
        "          .size()\n",
        "          .reset_index(name='{}transactions_count'.format('1')))\n",
        "\n",
        "agg_trans = pd.merge(df, agg_trans, on='location', how='left')\n",
        "train_eng = pd.merge(train_eng,agg_trans, on='location', how='left')\n",
        "train_eng.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce3c5d6",
      "metadata": {
        "id": "0ce3c5d6"
      },
      "outputs": [],
      "source": [
        "agg_trans = test_eng.groupby(['location']).agg(aggs)\n",
        "agg_trans.columns = ['_'.join(col).strip() for col in agg_trans.columns.values]\n",
        "agg_trans.reset_index(inplace=True)\n",
        "\n",
        "df = (test_eng.groupby('location')\n",
        "          .size()\n",
        "          .reset_index(name='{}transactions_count'.format('1')))\n",
        "\n",
        "agg_trans = pd.merge(df, agg_trans, on='location', how='left')\n",
        "test_eng = pd.merge(test_eng,agg_trans, on='location', how='left')\n",
        "test_eng.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58423d8",
      "metadata": {
        "id": "a58423d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "759fb4d9",
      "metadata": {
        "id": "759fb4d9"
      },
      "outputs": [],
      "source": [
        "# Selecting the independent variables and the target variable\n",
        "\n",
        "X = train_eng.drop(['ID_LAT_LON_YEAR_WEEK', 'location', 'emission'], axis = 1).fillna(0)\n",
        "y = train_eng.emission\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = SEED)\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "cb_model = CatBoostRegressor(iterations=10000,\n",
        "                             learning_rate=0.045,\n",
        "                             depth=8,\n",
        "                             eval_metric='RMSE',\n",
        "                             random_seed = SEED,\n",
        "                             bagging_temperature = 0.2,\n",
        "                             od_type='Iter',\n",
        "                             metric_period = 50,\n",
        "                             od_wait=300)\n",
        "cb_model.fit(X_train,y_train,\n",
        "             use_best_model=True,\n",
        "             verbose=50)\n",
        "\n",
        "# Making predictions\n",
        "y_pred = cb_model.predict(X_test)\n",
        "\n",
        "# Measuring the accuracy of the model\n",
        "print(f'RMSE Score: {mean_squared_error(y_test, y_pred, squared=False)}') # 23432.342352754695"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9852df",
      "metadata": {
        "id": "9e9852df"
      },
      "outputs": [],
      "source": [
        "# Analyse predictions\n",
        "pred_errors = X_test.copy()\n",
        "pred_errors['emission'] = y_test\n",
        "pred_errors['prediction'] = y_pred\n",
        "pred_errors['error'] = abs(pred_errors.prediction - pred_errors.emission)\n",
        "pred_errors = pred_errors[['latitude',\t'longitude',\t'year',\t'week_no', 'emission', 'prediction', 'error']]\n",
        "pred_errors.sort_values(by = 'error', ascending = False, inplace = True)\n",
        "pred_errors.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba52084",
      "metadata": {
        "id": "dba52084"
      },
      "outputs": [],
      "source": [
        "train.emission.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef0ec7a3",
      "metadata": {
        "id": "ef0ec7a3"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the test set\n",
        "\n",
        "test_df = test_eng.drop(['ID_LAT_LON_YEAR_WEEK', 'location'], axis = 1).fillna(0)\n",
        "predictions = cb_model.predict(test_df)\n",
        "\n",
        "# # Create a submission file\n",
        "sub_file = pd.DataFrame({'ID_LAT_LON_YEAR_WEEK': test_eng.ID_LAT_LON_YEAR_WEEK, 'emission': predictions})\n",
        "sub_file.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09df9a8",
      "metadata": {
        "id": "e09df9a8"
      },
      "outputs": [],
      "source": [
        "# Create file\n",
        "sub_file.to_csv('BaselineSubmission_G6.csv', index = False) # Download subfile and submit to zindi for scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fd7bdc",
      "metadata": {
        "id": "d4fd7bdc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}